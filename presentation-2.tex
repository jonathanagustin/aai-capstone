\documentclass[aspectratio=169]{beamer}

% Modern theme & fonts with T1 encoding
\usetheme{metropolis}
\usefonttheme{professionalfonts}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{bookmark}

% Metropolis adjustments
\metroset{progressbar=none, sectionpage=none}
\setbeamertemplate{footline}{}

% Colors and frame title formatting
\setbeamercolor{normal text}{fg=black,bg=white}
\setbeamercolor{background canvas}{bg=white}
\setbeamercolor{frametitle}{fg=black,bg=white}

% Center frame titles
\makeatletter
\setbeamertemplate{frametitle}{
  \nointerlineskip%
  \begin{beamercolorbox}[wd=\paperwidth, sep=0.3cm, center]{frametitle}%
    \usebeamerfont{frametitle}\insertframetitle\par%
    \if\insertframesubtitle\relax%
    \else%
      \vspace{0.5em}%
      {\usebeamerfont{framesubtitle}\insertframesubtitle\par}%
    \fi%
  \end{beamercolorbox}%
}
\makeatother

% Packages
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    bookmarksnumbered=true
}
\usepackage{etoolbox}
\usepackage{media9}
\usepackage{minted}
\usemintedstyle{friendly}
\usepackage{menukeys}

\title{Data Preparation \& Ethical Data Handling}
\subtitle{AI Masters Capstone Project - Presentation 2}
\author{Jonathan Agustin}
\date{November 2024}

\begin{document}

%------------------------------------------------------------
% Title Slide
%------------------------------------------------------------
% VOICEOVER:
% Welcome back to our journey toward building automated, ethically grounded ML pipelines.
% In our first presentation, we laid out why automation, fairness, accountability, and compliance matter at every stage. We examined legal precedents, ethical imperatives, and the need for robust infrastructure.
% Today, we zoom in on the data layer—where all ML begins. Data is not just numbers; it reflects people’s lives, societal structures, and historical decisions. How we handle data can reinforce or reduce inequities.
% We’ll start by exploring the importance of ethical data handling and how automation helps maintain data integrity at scale. Then, we’ll delve into advanced validation strategies, guided by thought leaders like Rachel Thomas (2017), ensuring we create realistic validation sets that genuinely measure how our models will perform in the wild.
% We’ll also examine privacy and compliance, showing how to integrate legal requirements (like GDPR) into your data pipeline. Finally, we’ll discuss detecting and mitigating biases before training, preventing the replication of harmful stereotypes.
% By the end of this session, you’ll have a comprehensive framework for preparing and validating data ethically, paving the way for fair and responsible models in future steps.

\maketitle

%------------------------------------------------------------
% Overview Slide
%------------------------------------------------------------
% VOICEOVER:
% Today’s agenda is packed with actionable insights:
% 1. The ethical dimension of data handling: how trust, fairness, and societal impact guide our decisions.
% 2. Automating preprocessing: ensuring every data cleaning step is reproducible, transparent, and consistent.
% 3. Data validation and quality checks integrated into automated workflows to catch issues early.
% 4. Privacy and regulatory compliance: embedding best practices to uphold user rights and follow laws like GDPR.
% 5. Creating an effective validation set as emphasized by Rachel Thomas (2017), who reminds us that a naive validation strategy can mislead our model assessment.
% 6. Identifying and reducing bias in datasets before they reach the modeling stage.
% Throughout, we’ll connect these steps to broader ethical principles and show how each technical choice shapes the fairness and accountability of the final ML system.

\begin{frame}{What We’ll Cover Today}
\begin{itemize}
\item Ethical data handling: fundamentals and importance
\item Automated preprocessing: cleaning, transforming, and validating data
\item Privacy \& compliance: embedding regulations into data pipelines
\item Creating effective validation sets (Thomas, 2017): beyond naive splits
\item Detecting \& mitigating dataset bias before model training
\end{itemize}

\emph{Our goal: robust, transparent, and ethical data pipelines that set the stage for trustworthy ML.}
\end{frame}

%------------------------------------------------------------
% Importance of Ethical Data Handling
%------------------------------------------------------------
% VOICEOVER:
% Ethical data handling isn’t an afterthought—it’s foundational. Every record might represent a person’s credit history, medical condition, or hiring decision. Mishandling this data can lead to real harm: privacy infringements, unfair rejections of job applications, or biased lending decisions.
% By embracing ethical principles, we foster trust. Users and regulators alike want assurance that AI products treat everyone fairly and respect their rights. Organizations that embrace ethical stewardship stand out, avoid costly litigation, and build sustainable user relationships.
% This approach isn’t just moral—it’s strategic. Ethical rigor in data handling can become a differentiator in a crowded marketplace, signaling that you value integrity as much as innovation.

\begin{frame}{Why Ethical Data Handling Matters}
\begin{itemize}
\item Data represents real human stories, with tangible impacts
\item Trust \& credibility: essential assets for long-term AI adoption
\item Preventing bias, respecting privacy, and ensuring fairness safeguards users and protects organizations
\end{itemize}

\emph{Ethical standards in data handling underpin the entire ML lifecycle.}
\end{frame}

%------------------------------------------------------------
% Automated Data Preprocessing Intro
%------------------------------------------------------------
% VOICEOVER:
% Real-world data is messy: missing values, inconsistent formats, outliers that can skew models. Manual preprocessing is error-prone and hard to scale. By automating these steps—cleaning, imputing, transforming—we ensure consistency across releases and traceability over time.
% Automated pipelines also reduce human bias. If one engineer consistently drops certain data points or encodes categories differently each time, subtle biases can creep in. Automation enforces a uniform approach, making it easier to audit and improve.

\begin{frame}{Automated Data Preprocessing}
\begin{itemize}
\item Standardize cleaning: detect and handle missing values, outliers, inconsistencies
\item Consistent transformations: one-hot encoding, scaling, normalization are reproducible
\item Transparent pipelines: every step documented, reducing human error and hidden bias
\end{itemize}

\emph{Automation turns data wrangling into a reliable, transparent foundation for fair ML.}
\end{frame}

%------------------------------------------------------------
% Techniques for Data Cleaning and Transformation
%------------------------------------------------------------
% VOICEOVER:
% Let’s consider a practical code snippet. Imagine you have a raw CSV with both numerical and categorical features. Some values are missing, some categories are unencoded, and numeric ranges vary widely.
% Using tools like scikit-learn’s `SimpleImputer`, `OneHotEncoder`, and `StandardScaler`, we can create a reproducible pipeline. This pipeline ensures that regardless of who runs it and when, the same input leads to the same well-prepared output.
% Over time, as data sources evolve or new features are added, you can update this pipeline in a version-controlled environment (like Git), maintaining a changelog and ensuring each modification is intentional and reviewed.

\begin{frame}{Practical Preprocessing Techniques}
\begin{minted}[fontsize=\small]{python}
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

df = pd.read_csv("raw_data.csv")

# Impute missing numeric values
numeric_cols = df.select_dtypes(include=['float','int']).columns
imputer = SimpleImputer(strategy='mean')
df[numeric_cols] = imputer.fit_transform(df[numeric_cols])

# Encode categorical variables
categorical_cols = df.select_dtypes(include=['object']).columns
encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
encoded = encoder.fit_transform(df[categorical_cols])
encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(categorical_cols))
df = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)

# Scale numeric features
scaler = StandardScaler()
df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
\end{minted}

\emph{This pipeline enforces uniform steps, improving fairness and reducing arbitrary choices.}
\end{frame}

%------------------------------------------------------------
% Data Validation and Quality Assurance
%------------------------------------------------------------
% VOICEOVER:
% Cleaning is a start, but how do we ensure ongoing data quality? Automated validation steps catch schema violations, unexpected values, or missing fields before training begins.
% Tools like `pandera` let you define a schema that your data must adhere to. If something’s off—like ages outside a plausible range or missing critical columns—your pipeline can halt and alert the team. This prevents garbage-in, garbage-out scenarios and ensures that models are trained only on data that meets your standards.
% Continual validation also supports transparency. When stakeholders ask, “How do we know the data was right?” you have a codified, automated verification process as evidence.

\begin{frame}{Data Quality Assurance}
\begin{minted}[fontsize=\small]{python}
import pandera as pa
from pandera.typing import DataFrame, Series

class InputSchema(pa.SchemaModel):
    age: Series[int] = pa.Field(ge=0, le=120)
    income: Series[float] = pa.Field(ge=0)
    # Add additional constraints as needed
    class Config:
        strict = True

try:
    validated_df = InputSchema.validate(df)
except pa.errors.SchemaError as e:
    print("Data validation failed:", e)
    # Halt pipeline, send alerts
\end{minted}

\emph{Automated checks ensure data integrity at every step, building trust in your pipeline.}
\end{frame}

%------------------------------------------------------------
% Creating a Good Validation Set (Incorporating Rachel Thomas, 2017)
%------------------------------------------------------------
% VOICEOVER:
% Once the data is clean and validated, how do we ensure our models generalize? Rachel Thomas (2017) underscores the importance of choosing the right validation set. A naive random split might not capture real-world conditions.
% The validation set helps us tune hyperparameters, select models, and understand performance before we ever touch the final test set. If it doesn’t reflect future conditions or production data patterns, you risk overestimating model quality.
% For instance, if your future data differs by including new user segments, temporal shifts, or evolving categories, a random split won’t simulate this. We must design validation schemes aligned with actual future scenarios.

\begin{frame}{Creating a Good Validation Set (Thomas, 2017)}
\begin{itemize}
\item Validation = key to honest model assessment
\item Must simulate future data conditions, not just be a random sample
\item Consider time splits, new user groups, and domain shifts to reflect true deployment scenarios
\end{itemize}

\emph{As Thomas (2017) notes, poor validation sets often cause failure when models meet the real world.}
\end{frame}

%------------------------------------------------------------
% When Random Subsets Are Not Enough
%------------------------------------------------------------
% VOICEOVER:
% Thomas (2017) gives concrete examples:
% - Time Series: If your model predicts future sales, don’t randomly pick validation rows from the entire dataset. Instead, use later time periods for validation, ensuring the model is tested on truly “future” data.
% - New Users or Items: If your production environment introduces new users or product categories not seen in training, random splits won’t catch that. You need to separate out these new entities into the validation set to replicate the challenge of novelty.
%
% By carefully constructing validation sets, you avoid false confidence. It ensures that the model’s impressive validation score actually translates into reliable performance in production.

\begin{frame}{When is a Random Subset Not Good Enough? (Thomas, 2017)}
\begin{itemize}
\item **Time Series**: Validate on data from a later time window
\item **New Entities**: If future data includes new customers not seen before, validation should too
\item **Domain Shifts**: Reflect changes in user behavior or market conditions in your validation strategy
\end{itemize}

\emph{Adopting these strategies reduces the risk of costly surprises post-deployment.}
\end{frame}

%------------------------------------------------------------
% Practical Examples: Time Series & New Entities
%------------------------------------------------------------
% VOICEOVER:
% Let’s put this into context:
% - In a Kaggle grocery sales forecasting challenge, if the training data runs from January 2013 to July 2017, and the final test period is August 2017, set aside the last two weeks of July for validation. This simulates the model’s ability to predict what happens after July ends.
% - In distracted driver detection problems, you might train on a set of drivers and validate on a completely different set of drivers. If the model excels only on people it has “seen” before, it won’t generalize.
%
% These scenarios highlight that good validation simulates real deployment challenges. That’s what gives you confidence that your model’s performance metrics aren’t an illusion.

\begin{frame}{Practical Examples (Thomas, 2017)}
\begin{itemize}
\item **Time Series (e.g., grocery sales)**:
  - Training: Jan 2013 - July 31, 2017
  - Validation: Aug 1 - Aug 15, 2017
  - Test: Aug 16 onward
\item **New Users (e.g., distracted driver)**:
  - Training: certain known drivers
  - Validation: entirely new, unseen drivers
\end{itemize}

\emph{This tailored approach ensures validation metrics align with real-world performance.}
\end{frame}

%------------------------------------------------------------
% Avoiding Overfitting to Public Benchmarks (Kaggle Context)
%------------------------------------------------------------
% VOICEOVER:
% Thomas (2017) also warns about overfitting to public leaderboards, such as those on Kaggle. The public leaderboard may represent only part of the test data, and if you tailor your model to those particular examples, you’ll perform poorly on the unseen private test portion.
% Creating a strong, representative validation set in your local environment lets you trust your improvements. You’re not just chasing a leaderboard score—you’re building a model that’s genuinely robust. This mindset generalizes beyond competitions to any production scenario: don’t tweak your model just to shine on one particular slice of data.

\begin{frame}{Kaggle Considerations (Thomas, 2017)}
\begin{itemize}
\item Kaggle’s public leaderboard is a partial view; don’t overfit to it
\item Your custom validation set should approximate true future data, not just mimic the public leaderboard
\item This ensures real, lasting performance gains rather than superficial improvements
\end{itemize}

\emph{A sound validation strategy is an investment in long-term model reliability.}
\end{frame}

%------------------------------------------------------------
% Ethical Data Management: Privacy & Regulations
%------------------------------------------------------------
% VOICEOVER:
% Beyond quality and validation, ethical data handling involves privacy protection. Regulatory frameworks like GDPR mandate certain rights and safeguards. Failing to comply can result in heavy penalties and damaged reputations.
% Embed privacy into your pipeline:
% - Anonymize or pseudonymize identifiers to avoid linking data back to individuals.
% - Enforce data minimization: only collect what you need.
% - Support user requests to access, correct, or delete their data.
%
% These aren’t just legal hoops; they maintain trust and user safety. Treat privacy as a cornerstone of your data pipeline design.

\begin{frame}{Privacy Protection and Compliance}
\begin{minted}[fontsize=\small]{python}
import hashlib

# Pseudonymize user IDs
if 'user_id' in df.columns:
    df['user_id_hashed'] = df['user_id'].apply(
        lambda x: hashlib.sha256(str(x).encode()).hexdigest()
    )
    df.drop(columns=['user_id'], inplace=True)

# Remove PII (Personally Identifiable Information)
pii_cols = ['name', 'email', 'phone_number']
df = df.drop(columns=[col for col in pii_cols if col in df.columns])
\end{minted}

\emph{Privacy-by-design: integrate compliance and respect for users at the data level.}
\end{frame}

%------------------------------------------------------------
% Detecting and Mitigating Bias
%------------------------------------------------------------
% VOICEOVER:
% Bias is often hidden in historical data, reflecting societal inequalities. If left unchecked, models trained on biased data perpetuate these injustices.
% Tools like IBM’s `aif360` can measure fairness metrics like disparate impact across protected groups. If bias is detected, techniques like reweighting or resampling help rebalance the dataset.
% The result: more equitable outcomes. By actively managing bias now, you prevent negative headlines, regulatory scrutiny, and harm to marginalized communities. Ethical pipelines don’t just measure fairness—they actively improve it.

\begin{frame}{Detecting and Mitigating Bias}
\begin{minted}[fontsize=\small]{python}
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing

data = BinaryLabelDataset(
    favorable_label=1,
    unfavorable_label=0,
    df=df,
    label_names=['approved_loan'],
    protected_attribute_names=['gender']
)

metric = BinaryLabelDatasetMetric(
    data,
    unprivileged_groups=[{'gender':0}],
    privileged_groups=[{'gender':1}]
)

if metric.disparate_impact() < 0.8:
    rw = Reweighing(
        unprivileged_groups=[{'gender':0}],
        privileged_groups=[{'gender':1}]
    )
    data_balanced = rw.fit_transform(data)
\end{minted}

\emph{Embedding fairness checks ensures our ML solutions serve society responsibly.}
\end{frame}

%------------------------------------------------------------
% Putting It All Together
%------------------------------------------------------------
% VOICEOVER:
% We’ve explored how ethical principles intertwine with technical rigor:
% - Ethical handling and privacy safeguards build trust.
% - Automated preprocessing and validation ensure consistent, transparent data transformations.
% - Careful validation set construction (Thomas, 2017) reflects real-world complexities, leading to realistic performance estimates.
% - Bias detection and mitigation tools foster fairness.
%
% Together, these practices form a cohesive strategy for data preparation. This sets a solid, ethical stage for the next phases—model training, deployment, and continuous monitoring—ensuring that the entire ML lifecycle remains aligned with core human values.

\begin{frame}{A Holistic Data Strategy}
\begin{itemize}
\item Integrate cleaning, validation, privacy, bias mitigation, and realistic validation strategies
\item Document processes for transparency and auditability
\item Lay a foundation that upholds fairness, compliance, and trust through the entire ML pipeline
\end{itemize}

\emph{Such an approach is not only ethically sound but also yields more robust, future-proof models.}
\end{frame}

%------------------------------------------------------------
% Conclusion & Next Steps
%------------------------------------------------------------
% VOICEOVER:
% We’ve established a robust, ethical data pipeline. Next, we’ll move to the modeling stage, where automated training, fairness checks during model selection, and ongoing monitoring come into play.
% The journey continues with the same guiding principles: transparency, fairness, compliance, and user-centric thinking. By starting with good data practices, we give our future models the best possible chance to perform responsibly and effectively.

\begin{frame}{Next Steps}
\begin{itemize}
\item Next Presentation: Automating Model Training \& Ethical Evaluation
\item Build upon today’s principles to ensure model choices and hyperparameter tuning respect the same ethical and rigorous standards
\end{itemize}

\emph{Our pipeline is now primed for fair and responsible model development.}
\end{frame}

%------------------------------------------------------------
% References Slide
%------------------------------------------------------------
% VOICEOVER:
% Before we wrap up, here are some key references:
% - Rachel Thomas (2017) provides a valuable blueprint for constructing effective validation sets, a crucial aspect of reliable ML evaluation.
% - The aif360 toolkit and pandera library are practical tools to bring fairness metrics and data validation into your workflow.
% - GDPR guidelines remind us that privacy isn’t optional; it’s legally mandated and ethically required.
%
% These resources help you solidify and continuously improve your data handling practices, ensuring that your pipeline remains aligned with best practices and evolving standards.

\begin{frame}{References}
\footnotesize
\begin{itemize}
\item Thomas, R. (2017). \textit{How (and why) to create a good validation set}. \url{https://rachel.fast.ai/posts/2017-11-13-validation-sets/}
\item \textit{aif360} toolkit: \url{https://github.com/Trusted-AI/AIF360}
\item \textit{pandera} library: \url{https://pandera.readthedocs.io/}
\item GDPR guidelines: \url{https://gdpr.eu/}
\end{itemize}
\end{frame}

\end{document}
